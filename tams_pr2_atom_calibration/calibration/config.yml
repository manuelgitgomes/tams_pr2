#
#           █████╗ ████████╗ ██████╗ ███╗   ███╗
#          ██╔══██╗╚══██╔══╝██╔═══██╗████╗ ████║
#          ███████║   ██║   ██║   ██║██╔████╔██║
#          ██╔══██║   ██║   ██║   ██║██║╚██╔╝██║
#   __     ██║  ██║   ██║   ╚██████╔╝██║ ╚═╝ ██║    _
#  / _|    ╚═╝  ╚═╝   ╚═╝    ╚═════╝ ╚═╝     ╚═╝   | |
#  | |_ _ __ __ _ _ __ ___   _____      _____  _ __| | __
#  |  _| '__/ _` | '_ ` _ \ / _ \ \ /\ / / _ \| '__| |/ /
#  | | | | | (_| | | | | | |  __/\ V  V / (_) | |  |   <
#  |_| |_|  \__,_|_| |_| |_|\___| \_/\_/ \___/|_|  |_|\_\
#  https://github.com/lardemua/atom

# This yaml file describes your calibration!

# This parameter is automatically filled by ATOM, please only change if you know what you are doing
package_name: "tams_pr2_atom_calibration"
# You can start by defining your robotic system.
# This is the URDF file (or xacro) that describes your robot.
# Every time a path to a file is requested you can use
#
#   - Absolute Path
#       Example 1: /home/user/ros_workspace/your_package/urdf/description.urdf.xacro
#       Example 2: file://home/user/ros_workspace/your_package/urdf/description.urdf.xacro
#
#   - Path Expansion
#       Example 1: ${HOME}/user/${YOUR_VARIABLE}/your_package/urdf/description.urdf.xacro
#       Example 2: ~/user/ros_workspace/your_package/urdf/description.urdf.xacro
#
#       NOTE: It is up to you to guarantee the environment variable exists.
#
#   - ROS Package Reference
#       Example: package://your_package/urdf/description.urdf.xacro
#
description_file: "package://tams_pr2_description/robots/tams_pr2.urdf.xacro"

# The calibration framework requires a bagfile to extract the necessary data for the calibration.
bag_file: "$ROS_BAGS/tams_pr2/train_2023-11-09-09-53-17.bag"
# bag_file: "$ROS_BAGS/pr2/train_2023-11-16-08-49-46.bag"

# You must define a frame of reference for the optimization process.
# It must exist in the transformation chains of all the sensors which are being calibrated.
world_link: "base_footprint"

# ATOM will calibrate the extrinsic parameters of your sensors.
# In this section you should discriminate the sensors that will be part of the calibrations.
sensors:
  # Each key will define a sensor and its name, which will be use throughout the calibration.
  # Each sensor definition must have the following properties:
  #       link:
  #           The frame of the sensor's data (i.e. the header.frame_id).
  #
  #       parent_link:
  #           The parent link of the transformation (i.e. link) to be calibrated.
  #
  #       child_link:
  #           This is the transformation (i.e. link) that we be optimized.
  #
  #       topic_name:
  #           Name of the ROS topic that contains the data produced by this sensor.
  #           If you are calibrating an camera, you should use the raw image produced by the
  #           sensors. Additionally, it the topic is an image it will automatically use the
  #           respective `camera_info` topic.
  # OPTIONAL
  #       throttle:
  #           Set throttle: desired_frame_rate. If you don't use throttle, collapse the throttle option
  #       modality:
  #           Set this to identify the sensor modality. If this flag is not set, the sensor will be
  #           identified by the message type.
  #           Current options: lidar3d, rgb, depth, lidar2d
  #
  # If you are using an image compressed topic such as:
  #   /world_camera/rgb/image_raw/compressed
  # you should not add the "compressed" part, use only:
  #   /world_camera/rgb/image_raw
  #
  azure_rgb_camera:
    link: "azure_kinect_rgb_camera_link"
    parent_link: "azure_kinect_camera_base"
    child_link: "azure_kinect_rgb_sensor"
    topic_name: "/azure_kinect/rgb/image_raw"
    modality: "rgb"

  azure_depth_camera:
    link: "azure_kinect_depth_camera_link"
    parent_link: "azure_kinect_camera_base"
    child_link: "azure_kinect_depth_sensor"
    topic_name: "/azure_kinect/depth/image_raw"
    modality: "depth"

  narrow_left_camera:
    link: "narrow_stereo_l_stereo_camera_optical_frame"
    parent_link: "narrow_stereo_l_stereo_camera_frame"
    child_link: "narrow_stereo_l_stereo_camera_optical_frame"
    topic_name: "/narrow_stereo/left/image_rect"
    modality: "rgb"

  narrow_right_camera:
    link: "narrow_stereo_r_stereo_camera_optical_frame"
    parent_link: "narrow_stereo_r_stereo_camera_frame"
    child_link: "narrow_stereo_r_stereo_camera_optical_frame"
    topic_name: "/narrow_stereo/right/image_rect"
    modality: "rgb"

  wide_left_camera:
    link: "wide_stereo_l_stereo_camera_optical_frame"
    parent_link: "wide_stereo_l_stereo_camera_frame"
    child_link: "wide_stereo_l_stereo_camera_optical_frame"
    topic_name: "/wide_stereo/left/image_rect"
    modality: "rgb"

  wide_right_camera:
    link: "wide_stereo_r_stereo_camera_optical_frame"
    parent_link: "wide_stereo_r_stereo_camera_frame"
    child_link: "wide_stereo_r_stereo_camera_optical_frame"
    topic_name: "/wide_stereo/right/image_rect"
    modality: "rgb"


# ATOM will calibrate the extrinsic parameters of your links.
# In this section you should discriminate the additional transformations that will be part of the calibrations.
# NOTE: Delete the "" after additional_tfs to prevent the error 'All mapping items must start at the same column YAML'
additional_tfs: ""
  # Each key will define a transformations and its name, which will be use throughout the calibration.
  # Each additional transformations definition must have the following properties:
  #
  #       parent_link:
  #           The parent link of the transformation (i.e. link) to be calibrated.
  #
  #       child_link:
  #           This is the transformation (i.e. link) that we be optimized.
  #
  # EXAMPLE:
  # base_footprint_to_base_link:
  #   parent_link: "base_footprint"
  #   child_link: "base_link"

joints:
  head_pan_joint:
    parent_link: "torso_lift_link"
    child_link: "head_pan_link"
    params_to_calibrate: ['position_bias']
  head_tilt_joint:
    parent_link: "head_pan_link"
    child_link: "head_tilt_link"
    params_to_calibrate: ['position_bias']
  l_shoulder_pan_joint:
    parent_link: "torso_lift_link"
    child_link: "l_shoulder_pan_link"
    params_to_calibrate: ['position_bias']
  l_shoulder_lift_joint:
    parent_link: "l_shoulder_pan_link"
    child_link: "l_shoulder_lift_link"
    params_to_calibrate: ['position_bias']
  l_upper_arm_roll_joint:
    parent_link: "l_shoulder_lift_link"
    child_link: "l_upper_arm_roll_link"
    params_to_calibrate: ['position_bias']
  l_elbow_flex_joint:
    parent_link: "l_upper_arm_link"
    child_link: "l_elbow_flex_link"
    params_to_calibrate: ['position_bias']
  l_forearm_roll_joint:
    parent_link: "l_elbow_flex_link"
    child_link: "l_forearm_roll_link"
    params_to_calibrate: ['position_bias']
  l_wrist_flex_joint:
    parent_link: "l_forearm_link"
    child_link: "l_wrist_flex_link"
    params_to_calibrate: ['position_bias']
  l_wrist_roll_joint:
    parent_link: "l_wrist_flex_link"
    child_link: "l_wrist_roll_link"
    params_to_calibrate: ['position_bias']


# The calibration requires a detectable pattern.
# This section describes the properties of the calibration pattern used in th calibration.
calibration_pattern:

  # The frame id (or link) of the pattern.
  # This link/transformation will be optimized.
  link: "pattern_link"

  # The parent frame id (or link) of the pattern.
  # For example, in hand-eye calibration the parent link
  # of the pattern can be the end-effector or the base of the arm
  parent_link: "l_gripper_l_finger_tip_frame"

  # Defines if the pattern link is the same in all collections (i.e. fixed=true),
  # or each collection will have its own estimate of the link transformation.
  # Note: if you plan to have the pattern fixed, while the moving the rigidly attached sensors,
  # this is equivalent to having the sensors fixed and the pattern moving, so you should use fixed=false.
  fixed: true

  # The type of pattern used for the calibration.
  # Supported pattern are:
  # - chessboard
  # - charuco
  pattern_type: "chessboard"

  # If the pattern type is "charuco" you need to define
  # the aruco dictionary used by the pattern.
  # See https://docs.opencv.org/trunk/dc/df7/dictionary_8hpp.html
  dictionary: ""

  # Mesh file (collada.dae or stl) for showing pattern on rviz. URI or regular path.
  # See: description_file
  mesh_file: "package://atom_worlds/pattern/models/chessboard_200x214_5x4/chessboard_200x214_5x4.dae"

  # The border width from the edge corner to the pattern physical edge.
  # Used for 3D sensors and lidars.
  # It can be a scalar (same border in x and y directions), or it can be {'x': ..., 'y': ,,,}
  border_size: { 'x': 0.03, 'y': 0.03 }

  # The number of corners the pattern has in the X and Y dimensions.
  # Note: The charuco detector uses the number of squares per dimension in its detector.
  # Internally we add a +1 to Y and X dimensions to account for that.
  # Therefore, the number of corners should be used even for the charuco pattern.
  dimension: { "x": 5, "y": 4 }

  # The length of the square edge.
  size: 0.025

  # The length of the charuco inner marker.
  inner_size: 0.015


# Miscellaneous configuration

# If your calibration problem is not fully constrained you should anchored one of the sensors.
# This makes it immovable during the optimization.
# This is typically referred to as gauge freedom.
anchored_sensor: ""

# Max time delta (in milliseconds) between sensor data messages when creating a collection.
max_duration_between_msgs: 1000

